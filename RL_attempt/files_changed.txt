imitation_learning, hindsight_experience replay - remove debug messages and make it load all data 
environment - make it save time 
imitation_learning - make it skip invalid tests, errors mm 
utils - phosphorus thing 
representations, agent - trying to remove that warning (representations is all commented since nth has to be changed in it) 
representations - replaced InstanceNorm1d in predictors with ReLU. This means, predictor won't work anymore, but fragment tree GCN is unaffected 

AND WE KIND OF DELETED IMITATION LEARNING WOW HAHA 

representations - allow undoing actions 
create file: non_anomalous_path.py 
shld delete imitaiton_learning.py, hindsight_experience_replay.py 
create file: astar_search.py 


TODO NUMBER 2: 
compare mass specs mm find out why the rmse is so suspiciously high - VERY IMPT MMMM 

figure out the one-class gnn thing - it seems simple actl. https://arxiv.org/pdf/2002.09594.pdf 
squared euclidean distance between pos and center of circle, subtracted by radius squared and clipped to 0, (so it'll consider outside), 
+ radius squared to penalize high radius, + weight stuff for weight decay 
look at OCGNN/optim/loss.py hehe 
for weight decay, just use optimizer torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay), mm. 
So, changes to make: change optimizer to that, change loss function. Wait, that's it? Predict radius something? 

also, for HER, if we want to be able to change the goal, we need to generate fragment tree of new stuff... how??? 
Or, otherwise, generate a replacement ftree embedding 
So.... no, we can't actually do it.. i mean we cld look at how to make ftree, does this work https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4509603/ 
Or what are we gonna do instead? 




hehe random fun stuff to expore 
https://towardsdatascience.com/why-adamw-matters-736223f31b5d 



